\begin{abstract}
	Randomized smoothing has emerged as a prevalent method for certified robustness. In this paper, we extend further into the transferability of randomized smoothing. 
	We introduce a simple yet effective pipeline that empowers pretrained robust models to defend against attacks on downstream tasks, requiring solely the fine-tuning on the clean datasets derived from these tasks. 
	Our approach relies on two key techniques.
	First, during pretraining, we mix noisy images of different intensities with clean ones, as the interaction between various noise strengths enhances the overall defensive capability.
	Second, we highlight the critical role of statistical information in normalization layers for successful transfer. 
	We provide a detailed investigation into the functioning mechanisms of these two critical techniques. 
	Our proposed transfer method can be broadly applied to multiple downstream tasks.
    By merely fine-tuning on clean images, we have achieved impressive accuracy and robustness across 12 downstream tasks.
\end{abstract}