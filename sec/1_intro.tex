\section{Introduction}
\label{sec:intro}

V2

The art of refining pre-trained foundation models with large datasets for task-specific applications has become the leading strategy in the AI landscape. 
These foundation models acquire rich representations from a broad data distribution during the pre-training phase.
Consequently, a slight fine-tuning is sufficient for these foundation models to competently adapt to various downstream tasks.
This paradigm has become prevalent in various domains, including but not limited to, natural language processing, computer vision, speech, and multimedia.
% Products of this paradigm such as GPT, LLama, Stable Diffusion, ControlNet, among others, have generated profound societal effects.


However, despite the appeal of these models, their inherent lack of robustness presents significant security threats.
Recent studies have shown that these models are vulnerable to adversarial attacks, which can be easily crafted by adding imperceptible perturbations to the input.
% These perturbations are often imperceptible to humans, but they can cause the model to make incorrect predictions.
This vulnerability has raised serious concerns about the safety of these models in real-world applications.
To address this issue, researchers have proposed numerous certified robustness methods, including randomized smoothing, interval bound propagation, and others.
Among these methods, randomized smoothing has gradually emerged as a mainstream approach.
Randomized smoothing trains models on random Gaussian noise and utilizes a smoothed classifier to obtain a certified radius. Within this radius, robustness is guaranteed.
Many works have elegantly extended and innovatively refined randomized smoothing, culminating in a steady stream of enhanced outcomes.


In this paper, our goal is to learn a certifiably robust model that can be readily finetuned for adaptation to a particular task.
A key challenge is dealing with the compromise between semantic learning and robustness.
Our findings suggest that expanding the data distribution at the pre-training stage is highly significant.
A broad data distribution enables the model to learn both rich semantic features and robustness simultaneously.
This makes it unnecessary to make trade-offs between semantic learning and robustness.
Such a model merely requires minor adjustments to adapt to downstream tasks.
To expand the data distribution, we present a simple, yet incredibly powerful strategy.
In particular, we pre-train on a mixture of clean and various noisy images.


Surprisingly, we found that such a mixed pre-training model, even when only fine-tuned on the clean images of downstream tasks, can achieve a strong level of certified robustness.
This indicates that the model has successfully transferred not only the semantic features but also robustness to the downstream tasks.
This allows for a fine-tuning process that is remarkably straightforward and efficient.
% As illustrated in Table 1, the distribution gap between the upstream and downstream datasets has a significant impact on transferability.
Existing works on randomized smoothing often sacrifice natural accuracy in order to achieve robust accuracy.
By contrast, we do not need to compromise on clean accuracy.
Our method is capable of achieving both high clean accuracy and robust accuracy simultaneously.


Another advantage of our strategy is that it only requires training one model to handle various noises, instead of training multiple models.
Existing research on randomized smoothing involves training a separate model for each specific noise.
Such trained models can only handle specific noise and struggle when faced with noise of different magnitudes.
Also, training multiple models complicates the model's deployment process.
In contrast, our approach needs only one model to deal with multiple types of noise. This simplifies the application of the model and drastically cuts computational costs, especially in the context of transfer learning across multiple downstream tasks.


The pre-training of our model facilitates its easy adaptation to multiple downstream tasks.
Compared to existing work, we achieve similar or even superior results with just one model, as opposed to their method of using several models.
Additionally, we performed extensive experiments to investigate the correlation between semantic learning and robustness during the model's transfer process.
We hope that these analyses could stimulate more exploration of robustness within the community.





\iffalse



V1

As deep learning models continue to permeate various fields, the attention towards adversarial attacks and defenses has significantly grown among researchers and practitioners. 
Among numerous certified robustness methods, randomized smoothing has gradually emerged as a mainstream approach.
Randomized smoothing trains models on random Gaussian noise and utilizes a smoothed classifier to obtain a certified radius. Within this radius, robustness is guaranteed.
Many works have elegantly extended and innovatively refined randomized smoothing, culminating in a steady stream of enhanced outcomes.

\begin{table}[t]
  \label{tab:sum}
  \centering
  \caption{Transferability of semantic and robustness.}
  \begin{tabular}{llll}
  \hline
  pretrain    & finetune   & Clean Acc. & $\mathcal{N}$ 0.25 Acc. \\ \hline
  Clean       & Clean      &            &                 \\
  Clean       & $\mathcal{N}$ 0.25 &            &                 \\
  $\mathcal{N}$ 0.25  & Clean      &            &                 \\
  $\mathcal{N}$ 0.25  & $\mathcal{N}$ 0.25 &            &                 \\
  Mixed Noise & Clean      &            &                 \\ \hline
  \end{tabular}
\end{table}


However, existing randomized smoothing methods entail considerable computational expenses.
Typical randomized smoothing algorithms require the infusion of abundant random Gaussian noise into each clean image during training.
In case of multiple datasets, these training efforts must be repeated for each dataset, thus inflating the training expenses considerably.
Moreover,  a model trained under a specific intensity of Gaussian noise can only resist noise of the similar intensity during inference, proving to be ineffective against noise of dissimilar intensities.
Present methods manage different noise levels by training multiple models under different noise intensities, thereby further escalating the training costs.


To mitigate computational costs, a natural approach would be to incorporate transferability into the robust model. 
This approach should facilitate the efficient adaptation of a robust model, pretrained on a relatively extensive dataset, to various downstream datasets.
Thus,  dealing with noisy images on each downstream dataset demands only a minimal amount of transfer costs.
Furthermore, if a model is capable of managing various intensities of noise, we can notably reduce the costs of both pretraining and fine-tuning.
Advancing further, it is critical to reduce the complexity and the training overhead during fine-tuning.
The most favorable situation would be to handle different noises by merely fine-tuning on clean images.


In this paper, we designed a simple yet highly efficient transfer approach for randomized smoothing.
Our robust model is capable of transferring to multiple downstream datasets, handling various noise intensities, and only requires minimal fine-tuning on clean images (with as few as one epoch) .
Specifically, during the pretraining process, we mix clean images with noisy images infused with different levels of noise . 
This process empowers a model to deal with noise of diverse intensities simultaneously.
Next, we conduct fine-tuning using purely clean images on multiple downstream datasets.
The success of the transfer hinges on two pivotal aspects. 
The first is the mixture of clean and various noisy images during the pretraining phase.
The second is the statistical information in the model's normalization layer.


The mixture of clean and various noisy images offers three distinct advantages.
Firstly, the interaction between clean and various noisy images can stimulate each other, functioning as a data augmentation technique.
Secondly, incorporating clean images into the training enables the model to learn strong semantic representations.
The model can achieve high accuracy on clean images in both the upstream and downstream stages, eliminating the need to make a trade-off between robustness and accuracy.
Thirdly, by encompassing a broader data distribution, the model's data distribution aligns more closely with the downstream clean image distribution. 
This makes the adaptation during fine-tuning remarkably easy. 
Even with just one epoch of fine-tuning on downstream clean images, the model can achieve excellent accuracy and robustness.


Under the situation of mixed noise, the normalization layer of the model plays a decisive role in the success of  transfer, but it is often overlooked.
Current work on randomized smoothing heavily uses ResNet, where the Batch Norm by default computes the training set's running mean and variance, which are then applied to the validation set and the downstream dataset.
We conducted extensive ablation studies with various normalization layers, discovering that these statistics severely hamper robust transfer capabilities when noise is mixed.
Our analysis indicates that the mixing of multiple noises increases the disparity in the noisy data distribution.
Therefore, it's crucial not to use these statistics when mixing noise.


We conducted mixed noise pretraining on ImageNet and then fine-tuned on clean images across twelve downstream datasets. With minimal fine-tuning cost, we obtained impressive results on all these datasets.

\fi


\begin{figure*}  
  \centering  
  % \includegraphics[width=0.5\textwidth]{sec/fig/CIFAR10_curve.pdf}  
    \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}} 
  \caption{The transferability of semantic and robustness.}  
  \label{fig:transfer}  
\end{figure*}  




\newpage
\null

\section{Related Work}
\label{sec:related_work}

\subsection{Randomized Smoothing}

\subsection{Robustness Transfer}


\newpage




\section{Method}
\label{sec:method}
Achieving certified robustness often comes at a high computational cost and at the expense of natural accuracy. We propose a robust transfer framework to address this issue.
Our process begins with pre-training on a relatively large dataset, incorporating various types of noise. This is followed by fine-tuning on clean images from the downstream task. This method ensures that semantic representations are transferred while retaining the model's robustness.
Our framework is not only simple but also highly efficient. Fine-tuning solely on clean images significantly reduces the computational cost and achieves high natural accuracy.


\subsection{Pipeline}
Our pipeline consists of two stages. In the pre-training stage, we allow the model to simultaneously learn semantic representation and robustness. Following this, in the fine-tuning stage, we ensure the model retains its robustness while performing semantic adaptation.

\textbf{Pretraining.}
In order to transfer both semantic representation and robustness to downstream datasets, it's necessary to enable the model to learn both elements during the pre-training stage. A natural idea is to blend various types of noise into the training process, using labels for supervision.
Given a set of noise S={0, n1, n2, … ni, …}, we uniformly select noise from this set at random. This noise is then added to a clean image, which is fed into the model for training. Since clean images (i.e., n = 0) occur randomly during the training process, the model is effectively able to learn the semantic representation.
Incorporating noisy images into the training set forces the model's decision boundary to include both clean and noisy representations of the same category. This enables the model to learn robustness.

\textbf{Fine-tuning.}
During fine-tuning on downstream datasets, we only use clean images and their labels from these datasets. This approach allows the model to quickly transfer the semantic representations learned during the pre-training phase to the downstream dataset. It also achieves excellent natural accuracy on the downstream dataset without compromising between semantic representation and robustness. Additionally, fine-tuning solely on clean images significantly reduces computational costs. We found that even just one epoch of fine-tuning can yield excellent clean accuracy and certified robustness on the downstream dataset.

\subsection{Manifold Gap}
To ensure robustness while transferring semantics, it is crucial to minimize the manifold gap between the pre-training dataset and fine-tuning dataset.
If the manifold gap between the pre-training and fine-tuning datasets is small, the model only needs to make minor adjustments on the manifold to adapt to the new dataset. Thus, both semantic representation and robustness can be transferred simultaneously.
However, if the manifold gap between the pre-training and fine-tuning datasets is large, the model will need to make a substantial shift on the manifold to adapt to the new dataset. This forces the model to make trade-offs between semantic representation and robustness.
We found that the set of noise used in pre-training and fine-tuning determines the manifold gap. If the sets of noise used in both the pre-training and fine-tuning stages overlap, the manifold gap is small. However, if there is no overlap in the sets of noise used in these two stages, the manifold gap becomes large.
To minimize the manifold gap between the pre-training and fine-tuning stages, we use a blend of multiple types of noise, i.e., S={0, n1, n2, … ni, …}, during the pre-training stage, while only using clean images, i.e., S={0}, during the fine-tuning stage. This approach allows for the simultaneous transfer of semantics and robustness. Moreover, the use of only clean images makes fine-tuning highly efficient and straightforward.

\subsection{Statistics Adaptation}

We discovered in our experiments that the statistics within the model play a crucial role in the transfer of semantics and robustness. Some normalization layers (e.g., Batch Norm) default to using the mean and variance statistically derived from one dataset for another dataset. We found that these statistics can significantly disrupt the transfer process.

In the pre-training stage, we mixed multiple types of noise, while in the fine-tuning stage, we only used clean images. The statistics (i.e., mean and variance) calculated by the normalization layer in these two stages differ significantly. If the statistics from the pre-training stage are used in the fine-tuning stage, it will cause confusion in the fine-tuning stage, thereby affecting the transfer of semantics and robustness.

We conducted extensive ablation studies on the statistics to elaborate further. To ensure the transfer of semantics and robustness, we used Layer Normalization and did not use statistics. Layer Normalization normalizes the inputs across the features instead of normalizing the inputs across batches like Batch Normalization. By eliminating the reliance on pre-computed statistics, it effectively reduces the manifold gap and thus facilitates the transfer of both semantics and robustness.


\newpage


\section{Experiments}
\label{sec:experiments}


\subsection{Settings}

\textbf{Datasets.}

\textbf{Models.}

\textbf{Metrics.}

\textbf{Implementation Details.}


\subsection{Ablation Study}

\textbf{Mixed Noise.}

\textbf{Statistics Adaptation.}


\subsection{Comparison with State-of-the-art Methods}

\textbf{CIFAR10.}


\subsection{Downstream Tasks}

\textbf{CIFAR100.}

\textbf{Other Datasets.}




\begin{table*}[t]
  \label{tab:comparison}
  \centering
  \caption{Comparison with SOTA.}
  \begin{tabular}{cc|c|cccc}
    \hline
    \multicolumn{2}{c|}{\multirow{2}{*}{Method}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Clean\\ Acc.\end{tabular}} & \multicolumn{4}{c}{Certified Acc. at $\varepsilon$ (\%)} \\ \cline{4-7} 
    \multicolumn{2}{c|}{}       &                                                                       & 0.25           & 0.5           & 0.75           & 1.0          \\ \hline
    \multicolumn{2}{c|}{PixelDP  \citeyear{lecuyer2019certified} \cite{lecuyer2019certified}} & -  &  $ \prescript{(71.0)}{}{22.0}$   &   $ \prescript{(44.0)}{}{2.0}$     &    -    &   -      \\
    \multicolumn{2}{c|}{Randomized Smoothing \citeyear{cohen2019certified} \cite{cohen2019certified}}  &  -  &  $ \prescript{(75.0)}{}{61.0}$   &   $ \prescript{(75.0)}{}{43.0}$     &    $ \prescript{(65.0)}{}{32.0}$    &   $ \prescript{(66.0)}{}{22.0}$      \\
    \multicolumn{2}{c|}{SmoothAdv \citeyear{salman2019provably} \cite{salman2019provably}} &  -  &  $ \prescript{(84.3)}{}{74.9}$   &   $ \prescript{(80.1)}{}{63.4}$     &    $ \prescript{(80.1)}{}{51.9}$    &   $ \prescript{(62.2)}{}{39.6}$      \\
    \multicolumn{2}{c|}{Consistency \citeyear{jeong2020consistency} \cite{jeong2020consistency}} &  -  &  $ \prescript{(77.8)}{}{68.8}$   &   $ \prescript{(75.8)}{}{58.1}$     &    $ \prescript{(72.9)}{}{48.5}$    &   $ \prescript{(52.3)}{}{37.8}$      \\
    \multicolumn{2}{c|}{MACER \citeyear{zhai2020macer} \cite{zhai2020macer} }  & -   &  $ \prescript{(81.0)}{}{71.0}$   &   $ \prescript{(81.0)}{}{59.0}$     &    $ \prescript{(66.0)}{}{46.0}$    &   $ \prescript{(66.0)}{}{38.0}$      \\
    \multicolumn{2}{c|}{Denoised \citeyear{salman2020denoised} \cite{salman2020denoised} }  & -   &  $ \prescript{(72.0)}{}{56.0}$   &   $ \prescript{(62.0)}{}{41.0}$     &    $ \prescript{(62.0)}{}{28.0}$    &   $ \prescript{(44.0)}{}{19.0}$      \\
    \multicolumn{2}{c|}{Boosting \citeyear{horvath2021boosting} \cite{horvath2021boosting} }  & -   &  $ \prescript{(83.4)}{}{70.6}$   &   $ \prescript{(76.8)}{}{60.4}$     &    $ \prescript{(71.6)}{}{52.4}$    &   $ \prescript{(52.4)}{}{38.8}$      \\
    \multicolumn{2}{c|}{DRT \citeyear{yang2021certified} \cite{yang2021certified} }  & -   &  $ \prescript{(81.5)}{}{70.4}$   &   $ \prescript{(72.6)}{}{60.2}$     &    $ \prescript{(71.9)}{}{50.5}$    &   $ \prescript{(56.1)}{}{39.8}$      \\
    \multicolumn{2}{c|}{SmoothMix \citeyear{jeong2021smoothmix} \cite{jeong2021smoothmix} }  & -   &  $ \prescript{(77.1)}{}{67.9}$   &   $ \prescript{(77.1)}{}{57.9}$     &    $ \prescript{(74.2)}{}{47.7}$    &   $ \prescript{(61.8)}{}{37.2}$      \\
    \multicolumn{2}{c|}{Lee \citeyear{lee2021provable} \cite{lee2021provable} }  & -   &  60.0   &   42.0     &    28.0    &   19.0      \\
    \multicolumn{2}{c|}{ACES \citeyear{horvath2022robust} \cite{horvath2022robust} }  & -   &  $ \prescript{(79.0)}{}{69.0}$   &   $ \prescript{(74.2)}{}{57.2}$     &    $ \prescript{(74.2)}{}{47.0}$    &   $ \prescript{(58.6)}{}{37.8}$      \\ \hline
    \multicolumn{2}{c|}{\textbf{Ours}}  & \textbf{97.7}  &  $ \prescript{(\textbf{89.3})}{}{\textbf{77.7}}$   &   $ \prescript{(89.3)}{}{62.3}$     &    $ \prescript{(89.3)}{}{46.3}$    &   $ \prescript{(76.4)}{}{31.4}$      \\ \hline
 \end{tabular}
\end{table*}



\begin{table*}[t]
  \label{tab:cifar10}
  \centering
  \caption{Experiments on CIFAR10.}
  \begin{tabular}{cc|c|cccc}
    \hline
    \multicolumn{2}{c|}{\multirow{2}{*}{Train}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Clean\\ Acc.\end{tabular}} & \multicolumn{4}{c}{Certified Acc. at $\varepsilon$ (\%)} \\ \cline{4-7} 
    \multicolumn{2}{c|}{}       &                                                                       & 0.25           & 0.5           & 0.75           & 1.0          \\ \hline
    \multicolumn{2}{c|}{Clean} & 87.5  &  $ \prescript{(10.5)}{}{10.5}$   &   $ \prescript{(10.5)}{}{10.1}$     &    $ \prescript{(10.5)}{}{9.7}$    &   $ \prescript{(10.5)}{}{5.5}$      \\
    \multicolumn{2}{c|}{$\mathcal{N}$ 0.25}  &  71.8  &  $ \prescript{(74.2)}{}{59.3}$   &   $ \prescript{(74.2)}{}{41.7}$     &    $ \prescript{(74.2)}{}{26.0}$    &   $ \prescript{(46.0)}{}{6.7}$      \\
    \multicolumn{2}{c|}{$\mathcal{N}$ 0.5} &  54.9  &  $ \prescript{(61.9)}{}{49.0}$   &   $ \prescript{(61.9)}{}{38.7}$     &    $ \prescript{(61.9)}{}{29.6}$    &   $ \prescript{(61.9)}{}{20.8}$      \\
    \multicolumn{2}{c|}{$\mathcal{N}$ 1.0} &  41.1  &  $ \prescript{(45.0)}{}{39.1}$   &   $ \prescript{(45.0)}{}{30.0}$     &    $ \prescript{(45.0)}{}{24.8}$    &   $ \prescript{(40.9)}{}{19.8}$      \\
    \multicolumn{2}{c|}{Mixed $\mathcal{N}$}  & 75.9   &  $ \prescript{(66.3)}{}{51.6}$   &   $ \prescript{(66.3)}{}{36.5}$     &    $ \prescript{(56.5)}{}{26.0}$    &   $ \prescript{(41.5)}{}{17.7}$      \\ \hline
    \multirow{2}{*}{Pretrain}   & \multirow{2}{*}{Finetune}  & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Clean\\ Acc.\end{tabular}} & \multicolumn{4}{c}{Certified Acc. at $\varepsilon$ (\%)} \\ \cline{4-7} 
                                &                            &                                                                       & 0.25           & 0.5           & 0.75           & 1.0          \\ \hline
    Clean                       & Clean                      & 97.8  &  $ \prescript{(10.1)}{}{10.1}$   &   $ \prescript{(10.1)}{}{10.1}$     &    $ \prescript{(10.1)}{}{10.1}$    &   $ \prescript{(10.1)}{}{10.1}$      \\
    $\mathcal{N}$ 0.25                     & $\mathcal{N}$ 0.25                    & 82.7  &  $ \prescript{(89.7)}{}{78.6}$   &   $ \prescript{(89.7)}{}{57.1}$     &    $ \prescript{(89.7)}{}{38.5}$    &   $ \prescript{(46.6)}{}{11.7}$      \\
    $\mathcal{N}$ 0.5                      & $\mathcal{N}$ 0.5                     & 64.1  &  $ \prescript{(74.2)}{}{62.3}$   &   $ \prescript{(74.2)}{}{48.8}$     &    $ \prescript{(74.2)}{}{36.1}$    &   $ \prescript{(74.2)}{}{25.2}$      \\
    $\mathcal{N}$ 1.0                      & $\mathcal{N}$ 1.0                     & 38.8  &  $ \prescript{(52.2)}{}{42.1}$   &   $ \prescript{(52.2)}{}{34.1}$     &    $ \prescript{(52.2)}{}{27.4}$    &   $ \prescript{(52.2)}{}{22.0
    }$      \\
    Mixed $\mathcal{N}$                    & Clean                      & 98.0  &  $ \prescript{(85.5)}{}{71.4}$   &   $ \prescript{(85.5)}{}{49.0}$     &    $ \prescript{(85.5)}{}{25.4}$    &   $ \prescript{(39.1)}{}{17.7}$      \\ \hline
    Mixed $\mathcal{N}$ e400                    & Clean                      & 98.2  &  $ \prescript{(85.5)}{}{72.8}$   &   $ \prescript{(85.5)}{}{50.8}$     &    $ \prescript{(85.5)}{}{29.2}$    &   $ \prescript{(40.7)}{}{16.9}$      \\ \hline
    Mixed $\mathcal{N}$ e400                    & Clean e400                      & 98.1  &  $ \prescript{(89.3)}{}{76.0}$   &   $ \prescript{(89.3)}{}{53.3}$     &    $ \prescript{(89.3)}{}{32.6}$    &   $ \prescript{(73.2)}{}{20.1}$      \\ \hline
    Mixed $\mathcal{N}$ KL e400                  & Clean                      & 98.4  &  $ \prescript{(82.1)}{}{68.5}$   &   $ \prescript{(82.1)}{}{50.2}$     &    $ \prescript{(82.1)}{}{30.6}$    &   $ \prescript{(57.3)}{}{18.1}$      \\ \hline
    Mixed $\mathcal{N}$                  & Clean e1                     & 96.8  &  $ \prescript{(81.3)}{}{66.7}$   &   $ \prescript{(81.3)}{}{45.0}$     &    $ \prescript{(81.3)}{}{27.2}$    &   $ \prescript{(60.9)}{}{16.9}$      \\ \hline
    Clean   &  Mixed $\mathcal{N}$     & 96.9  &  $ \prescript{(70.4)}{}{55.8}$   &   $ \prescript{(58.3)}{}{38.7}$     &    $ \prescript{(58.3)}{}{27.6}$    &   $ \prescript{(58.3)}{}{17.3}$      \\ \hline
    Mixed $\mathcal{N}$ e400   &  Mixed $\mathcal{N}$ e800    & 98.1  &  $ \prescript{(90.6)}{}{78.1}$   &   $ \prescript{(90.6)}{}{61.3}$     &    $ \prescript{(90.6)}{}{45.3}$    &   $ \prescript{(77.0)}{}{31.1}$      \\ \hline
    Mixed $\mathcal{N}$ e400   &  Mixed $\mathcal{N}$ p1234 e400    & 98.0  &  $ \prescript{(90.4)}{}{78.5}$   &   $ \prescript{(90.4)}{}{62.1}$     &    $ \prescript{(90.4)}{}{44.1}$    &   $ \prescript{(77.3)}{}{32.0}$      \\ \hline
    Mixed $\mathcal{N}$ e400   &  Mixed $\mathcal{N}$ p1234 e800    & 97.7  &  $ \prescript{(89.3)}{}{77.7}$   &   $ \prescript{(89.3)}{}{62.3}$     &    $ \prescript{(89.3)}{}{46.3}$    &   $ \prescript{(76.4)}{}{31.4}$      \\ \hline
    Mixed $\mathcal{N}$ e400 s224   &  Mixed $\mathcal{N}$ e800 s224   & 98.1  &  $ \prescript{(90.4)}{}{79.5}$   &   $ \prescript{(90.4)}{}{62.7}$     &    $ \prescript{(90.4)}{}{43.2}$    &   $ \prescript{(77.7)}{}{31.8}$      \\ \hline
    \end{tabular}
\end{table*}



\begin{table*}[t]
  \label{tab:cifar100}
  \centering
  \caption{Experiments on CIFAR100.}
  \begin{tabular}{cc|c|cccc}
    \hline
    \multicolumn{2}{c|}{\multirow{2}{*}{Train}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Clean\\ Acc.\end{tabular}} & \multicolumn{4}{c}{Certified Acc. at $\varepsilon$ (\%)} \\ \cline{4-7} 
    \multicolumn{2}{c|}{}       &                                                                       & 0.25           & 0.5           & 0.75           & 1.0          \\ \hline
    \multicolumn{2}{c|}{Clean}              & 63.7    & $ \prescript{(3.2)}{}{1.4}$      & $ \prescript{(3.2)}{}{0.8}$          & $ \prescript{(0.6)}{}{0.6}$  &    $ \prescript{(0.6)}{}{0.6}$         \\
    \multicolumn{2}{c|}{$\mathcal{N}$ 0.25}  &  29.5  &  $ \prescript{(47.5)}{}{33.6}$   &   $ \prescript{(47.5)}{}{22.8}$     &    $ \prescript{(47.5)}{}{12.4}$    &   $ \prescript{(20.9)}{}{3.2}$      \\
    \multicolumn{2}{c|}{$\mathcal{N}$ 0.5} &  22.3  &  $ \prescript{(36.0)}{}{27.5}$   &   $ \prescript{(36.0)}{}{22.1}$     &    $ \prescript{(36.0)}{}{15.8}$    &   $ \prescript{(36.0)}{}{11.5}$      \\
    \multicolumn{2}{c|}{$\mathcal{N}$ 1.0} & 14.29  &  $ \prescript{(18.1)}{}{15.2}$   &   $ \prescript{(18.7)}{}{13.1}$     &    $ \prescript{(18.7)}{}{10.7}$    &   $ \prescript{(18.7)}{}{8.8}$      \\
    \multicolumn{2}{c|}{Mixed $\mathcal{N}$}  & 50.78  &  $ \prescript{(42.6)}{}{31.6}$   &   $ \prescript{(42.6)}{}{21.0}$     &    $ \prescript{(33.2)}{}{13.9}$    &   $ \prescript{(33.2)}{}{10.8}$      \\ \hline
    \multirow{2}{*}{Pretrain}   & \multirow{2}{*}{Finetune}  & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Clean\\ Acc.\end{tabular}} & \multicolumn{4}{c}{Certified Acc. at $\varepsilon$ (\%)} \\ \cline{4-7} 
                                &                            &                                                                       & 0.25           & 0.5           & 0.75           & 1.0          \\ \hline
    % Clean                       & Clean                      &   &  $ \prescript{()}{}{}$   &   $ \prescript{()}{}{}$     &    $ \prescript{()}{}{}$    &   $ \prescript{()}{}{}$      \\
    % $\mathcal{N}$ 0.25                     & $\mathcal{N}$ 0.25                    &   &  $ \prescript{()}{}{}$   &   $ \prescript{()}{}{}$     &    $ \prescript{()}{}{}$    &   $ \prescript{()}{}{}$      \\
    % $\mathcal{N}$ 0.5                      & $\mathcal{N}$ 0.5                     &   &  $ \prescript{()}{}{}$   &   $ \prescript{()}{}{}$     &    $ \prescript{()}{}{}$    &   $ \prescript{()}{}{}$      \\
    % $\mathcal{N}$ 1.0                      & $\mathcal{N}$ 1.0                     &   &  $ \prescript{()}{}{}$   &   $ \prescript{()}{}{}$     &    $ \prescript{()}{}{}$    &   $ \prescript{()}{}{}$      \\
    Mixed $\mathcal{N}$                    & Clean                      & 86.1  &  $ \prescript{(58.7)}{}{40.5}$   &   $ \prescript{(58.7)}{}{25.8}$     &    $ \prescript{(58.7)}{}{14.9}$    &   $ \prescript{(30.7)}{}{7.5}$      \\ \hline
    \end{tabular}
\end{table*}



\begin{table*}[]
  \label{tab:mixed_ablation}
  \centering
  \caption{Ablation study on mixed Noise.}
  \begin{tabular}{cc|c|cccc}
    \hline
   \multirow{2}{*}{Pretrain}   & \multirow{2}{*}{Finetune}  & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Clean\\ Acc.\end{tabular}} & \multicolumn{4}{c}{Certified Acc. at $\varepsilon$ (\%)} \\ \cline{4-7} 
                                &                            &                                                                       & 0.25           & 0.5           & 0.75           & 1.0          \\ \hline
    \multirow{4}{*}{Clean}                 & Clean                & 97.8  &  $ \prescript{(10.1)}{}{10.1}$   &   $ \prescript{(10.1)}{}{10.1}$     &    $ \prescript{(10.1)}{}{10.1}$    &   $ \prescript{(10.1)}{}{10.1}$      \\
                         & $\mathcal{N}$ 0.25                     & 77.6  &  $ \prescript{(83.7)}{}{69.4}$   &   $ \prescript{(83.7)}{}{53.0}$     &    $ \prescript{(83.7)}{}{34.1}$    &   $ \prescript{(50.6)}{}{10.9}$      \\
                          & $\mathcal{N}$ 0.5                     & 63.4  &  $ \prescript{(64.7)}{}{54.2}$   &   $ \prescript{(64.7)}{}{39.1}$     &    $ \prescript{(64.7)}{}{28.6}$    &   $ \prescript{(64.7)}{}{19.8}$      \\
                          & $\mathcal{N}$ 1.0                     & 39.3  &  $ \prescript{(42.5)}{}{35.7}$   &   $ \prescript{(42.5)}{}{30.2}$     &    $ \prescript{(42.5)}{}{23.4}$    &   $ \prescript{(42.5)}{}{19.4}$      \\ \hline

    \multirow{4}{*}{$\mathcal{N}$ 0.25}              & Clean                      & 94.4  &  $ \prescript{(42.9)}{}{21.6}$   &   $ \prescript{(42.9)}{}{10.7}$     &    $ \prescript{(12.1)}{}{7.5}$    &   $ \prescript{(12.1)}{}{4.2}$      \\
                                          & $\mathcal{N}$ 0.25                    & 82.7  &  $ \prescript{(89.7)}{}{78.6}$   &   $ \prescript{(89.7)}{}{57.1}$     &    $ \prescript{(89.7)}{}{38.5}$    &   $ \prescript{(58.3)}{}{11.7}$      \\
                                          & $\mathcal{N}$ 0.5                     & 64.7  &  $ \prescript{(76.0)}{}{65.7}$   &   $ \prescript{(76.0)}{}{51.0}$     &    $ \prescript{(76.0)}{}{38.5}$    &   $ \prescript{(76.0)}{}{25.8}$      \\
                                          & $\mathcal{N}$ 1.0                     & 54.3  &  $ \prescript{(63.1)}{}{52.6}$   &   $ \prescript{(61.1)}{}{43.5}$     &    $ \prescript{(61.1)}{}{31.5}$    &   $ \prescript{(61.1)}{}{23.4}$      \\ \hline

    \multirow{5}{*}{Mixed $\mathcal{N}$}              & Clean                      & 98.0  &  $ \prescript{(85.5)}{}{71.4}$   &   $ \prescript{(85.5)}{}{49.0}$     &    $ \prescript{(85.5)}{}{25.4}$    &   $ \prescript{(39.1)}{}{17.7}$      \\
                                          & $\mathcal{N}$ 0.25                    & 96.4  &  $ \prescript{(90.5)}{}{78.8}$   &   $ \prescript{(90.5)}{}{58.1}$     &    $ \prescript{(90.5)}{}{37.1}$    &   $ \prescript{(71.6)}{}{17.5}$      \\
                                          & $\mathcal{N}$ 0.5                     & 94.6  &  $ \prescript{(86.9)}{}{73.8}$   &   $ \prescript{(86.9)}{}{57.9}$     &    $ \prescript{(76.6)}{}{37.9}$    &   $ \prescript{(76.6)}{}{26.6}$      \\
                                          & $\mathcal{N}$ 1.0                     & 93.7  &  $ \prescript{(82.7)}{}{69.6}$   &   $ \prescript{(82.7)}{}{55.6}$     &    $ \prescript{(82.7)}{}{35.3}$    &   $ \prescript{(67.9)}{}{26.0}$      \\ 
                                          & Mixed $\mathcal{N}$                     & 97.9  &  $ \prescript{(88.7)}{}{77.6}$   &   $ \prescript{(88.7)}{}{57.5}$     &    $ \prescript{(88.7)}{}{36.7}$    &   $ \prescript{(72.6)}{}{23.8}$      \\ \hline

 
  \end{tabular}
\end{table*}

% \begin{table}[]
%   \label{tab:mixed_ablation}
%   \centering
%   \caption{Ablation study on mixed Noise.}
%   \begin{tabular}{cccccc}
%   \hline
%   \multirow{2}{*}{Pretrain}                                                             & \multirow{2}{*}{Finetune} & \multicolumn{4}{c}{Test Acc.}              \\ \cline{3-6} 
%                                                                                             &                             & Clean & N. 0.25 & N. 0.5 & N. 1.0 \\ \hline
%   \multirow{4}{*}{Clean}                                                                   & Clean                       &       &            &           &           \\
%                                                                                             & N. 0.25                  &       &            &           &           \\
%                                                                                             & N. 0.5                   &       &            &           &           \\
%                                                                                             & N. 1.0                   &       &            &           &           \\ \hline
%   \multirow{4}{*}{N. 0.25}                                                              & Clean                       &       &            &           &           \\
%                                                                                             & N. 0.25                  &       &            &           &           \\
%                                                                                             & N. 0.5                   &       &            &           &           \\
%                                                                                             & N. 1.0                   &       &            &           &           \\ \hline
%   \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Mixed N.\\  (0, 0.25)\end{tabular}}        & Clean                       &       &            &           &           \\
%                                                                                             & N. 0.25                  &       &            &           &           \\
%                                                                                             & N. 0.5                   &       &            &           &           \\
%                                                                                             & N. 1.0                   &       &            &           &           \\ \hline
%   \multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Mixed N.\\ (0, 0.25, \\ 0.5,1.0)\end{tabular}} & Clean                       &       &            &           &           \\
%                                                                                             & N. 0.25                  &       &            &           &           \\
%                                                                                             & N. 0.5                   &       &            &           &           \\
%                                                                                             & N. 1.0                   &       &            &           &           \\
%                                                                                             & Mixed N.                 &       &            &           &           \\ \hline
%   \end{tabular}
% \end{table}


\newpage
\null
\newpage


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \begin{table}[]
%   \label{tab:unsupervised}
%   \centering
%   \caption{Experiments on unsupervised pretraining.}
%   \begin{tabular}{cccccc}
%   \hline
%   \multicolumn{2}{c}{\multirow{2}{*}{Train from Scratch}}                                        & \multicolumn{4}{c}{Test Acc.}              \\ \cline{3-6} 
%   \multicolumn{2}{c}{}                                               & Clean & N. 0.25 & N. 0.5 & N. 1.0 \\ \hline
%   \multicolumn{2}{c}{Clean}                                                                      &       &            &           &           \\
%   \multicolumn{2}{c}{N. 0.25}                                                                 &       &            &           &           \\
%   \multicolumn{2}{c}{N. 0.5}                                                                  &       &            &           &           \\
%   \multicolumn{2}{c}{N. 1.0}                                                                  &       &            &           &           \\
%   \multicolumn{2}{c}{Mixed N.}                                                                &       &            &           &           \\ \hline
%   \multirow{2}{*}{Pretrain}                                          & \multirow{2}{*}{Finetune} & \multicolumn{4}{c}{Test Acc}               \\ \cline{3-6} 
%                                                                      &                           & Clean & N. 0.25 & N. 0.5 & N. 1.0 \\ \hline
%   \begin{tabular}[c]{@{}c@{}}Mixed N.\\ Supervised\end{tabular}   & Clean                     &       &            &           &           \\ \hline
%   \begin{tabular}[c]{@{}c@{}}Mixed N.\\ Unsupervised\end{tabular} & Clean                     &       &            &           &           \\ \hline
%   \end{tabular}
% \end{table}


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \begin{table*}[]
%   \label{tab:downstream}
%   \centering
%   \caption{Experiments on downstream datasets.}
%   \begin{tabular}{cl|cccc|llll}
%   \hline
%   \multicolumn{2}{c|}{\multirow{2}{*}{Train from Scratch}} & \multicolumn{4}{c|}{CalTech101 Test Acc.}                                                     & \multicolumn{4}{c}{CalTech256 Test Acc.}                                                                                     \\ \cline{3-10} 
%   \multicolumn{2}{c|}{}                 & Clean                & N. 0.25           & N. 0.5            & N. 1.0             & \multicolumn{1}{c}{Clean} & \multicolumn{1}{c}{N. 0.25} & \multicolumn{1}{c}{N. 0.5} & \multicolumn{1}{c}{N. 1.0} \\ \hline
%   \multicolumn{2}{c|}{Clean}                               &                      &                      &                      &                       &                           &                                &                               &                               \\
%   \multicolumn{2}{c|}{N. 0.25}                          &                      &                      &                      &                       &                           &                                &                               &                               \\
%   \multicolumn{2}{c|}{N. 0.5}                           &                      &                      &                      &                       &                           &                                &                               &                               \\
%   \multicolumn{2}{c|}{N. 1.0}                           &                      &                      &                      &                       &                           &                                &                               &                               \\ \hline
%   \multicolumn{2}{l|}{Mixed N. Transfer}                & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{} &                           &                                &                               &                               \\ \hline
%   \end{tabular}
% \end{table*}


\newpage
\null
\newpage
\null

\begin{figure*}
  \centering
  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1.0\textwidth]{sec/fig/CIFAR10_curve.pdf}   
    % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \caption{CIFAR10 Experiments.}
    \label{fig:cifar10 exp}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1.0\textwidth]{sec/fig/CIFAR100_curve.pdf}   
    % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \caption{CIFAR100 Experiments.}
    \label{fig:cifar100 exp}
  \end{subfigure}
  \caption{Vis.}
  \label{fig:vis}
\end{figure*}



\section{Conclusion}
\label{sec:conclusion}




